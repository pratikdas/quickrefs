# Deep Learning
## Activation functions
purpose: Add non-linearity to processing of inputs with weights and biases.

ReLU: Most commonly used in hidden layers. low compute resources required. y=max(0.0,x)

Logistic(Sigmoid): 
y=1.0 / (1.0 + e^-x)

Tanh:
y=(e^x â€“ e^-x) / (e^x + e^-x)


## Softmax function:
Used in output layer. Assign probabilities for classification.
